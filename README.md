# MNIST Neural Network from Scratch

## Overview

This project implements a neural network built from scratch to classify handwritten digits using the MNIST dataset. Built entirely with numpy for matrix operations and pandas for data handling, the model achieves approximately 94% accuracy on a 10,000-image test set. This hands-on approach allowed me to deepen my understanding of both linear algebra and neural network theory.

## Key Features and Model Architecture
Custom Neural Network Architecture: Flexible network with adjustable layers and activation functions.

Batch Training and Gradient Descent: Implements efficient matrix-based calculations.

The neural network is structured as follows:

Input Layer: Accepts flattened 28x28 images.

Hidden Layers: Configurable fully-connected layers with ReLU activation.

Output Layer: Softmax layer classifying digits from 0 to 9.

## Forward Propagation Equations
The core calculations in the network include:

**1.Linear Transformation:**

ğ‘ = ğ‘‹ â‹… ğ‘Š + ğµ
where:

 - ğ‘‹ is the input data matrix
 - ğ‘Š is the weight matrix for each layer
 - ğµ is the bias vector


**2. Activation (ReLU):**


 A = ReLU(ğ‘) = max(0,ğ‘)
â¡

3. Softmax Output:

Softmax(ğ‘)= 
âˆ‘e 
Z
e 
Z
 
â€‹
This transforms the outputs into probabilities, summing to 1 for classification.

##Loss Function
The model minimizes cross-entropy loss:

Loss
=
âˆ’
1
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
ğ‘¦
ğ‘–
â‹…
log
â¡
(
ğ‘¦
^
ğ‘–
)
Loss=âˆ’ 
m
1
â€‹
  
i=1
âˆ‘
m
â€‹
 y 
i
â€‹
 â‹…log( 
y
^
â€‹
  
i
â€‹
 )
where:

ğ‘š
m is the number of samples
ğ‘¦
ğ‘–
y 
i
â€‹
  is the true label
ğ‘¦
^
ğ‘–
y
^
â€‹
  
i
â€‹
  is the predicted probability for the true class
        
## Results
The model achieved approximately 94% accuracy on the MNIST test set, showcasing its capacity to generalize well on unseen data.

## Credits
A huge thank you to Samson Zhang for his educational videos that were instrumental in guiding this project.
